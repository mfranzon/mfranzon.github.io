[{"content":"Pods are, by default, able to connect each other. No matter if they run in the same or different namespaces, no NAT will be performed between pods. Check this article to have more info on pods network.\nTo change this behaviour it is necessary introduce a custom NetworkPolicy. Just to recall the definition:\n NetworkPolicy is a resource which gives you the possibility to specify how a pod communicates with other pods/services in the network.\n Here an example of a very generic NetworkPolicy. It declares, in the namespace you have choose, that no one can connect to any pod into that namespace.\napiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-communication spec: podSelector: If the previous is too strict you can introduce more informations. Assume that in a certain namespace you have a pod for the database, a pod for the web-app and other few pods. Probably, you want that the web-app can communicate with the database, but you do not want to give the same possibility to other pods. What you need is a NetworkPolicy that sets more specific rules.\nHere an example, using 5000 as communication port between web-app and database.\napiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: db-webapp spec: podSelector: matchLabels: app: database ingress: - from: - podSelector: matchLabels: app: webapp ports: - port: 5000 This NetworkPolicy has as subject the app : database resource, which means the pod with the database service. It enforces a communication between the two pods on the port 5000, without using services. More in detail, it create an ingress for the database service which accepts request just from web-app service. If there is also a service associated to those pods, this new rule will be add.\n","permalink":"https://kubetips.cloud/posts/isolatedns/","summary":"Pods are, by default, able to connect each other. No matter if they run in the same or different namespaces, no NAT will be performed between pods. Check this article to have more info on pods network.\nTo change this behaviour it is necessary introduce a custom NetworkPolicy. Just to recall the definition:\n NetworkPolicy is a resource which gives you the possibility to specify how a pod communicates with other pods/services in the network.","title":"NetworkPolicy : Enabling network isolation between Pods in the same Namespace"},{"content":"During the design phase you have to make a lot of right choices to avoid bad surprises during the deploy. One of the most frequent question is: \u0026ldquo;How can I organise my application in Kubernetes to achieve better performances ?\u0026rdquo; Sometimes the correct answer is just make the right decision in splitting and grouping the services in the correct way.\nPods, the resource you always need to Pods are the most used resource in a Kubernetes environment. They contains the running processes, containers to be more precise, one or more, and they could be the actuation of another resource. If you ever asked yourself why we need pods and why we do not use just containers, the answer is the container definition itself. A container is designed to run only a single process; if you decide to run multiple processes on the same container, you have to be sure that the processes in it are running and correct manage their logs because By default, the processes log to the same stdout.\nFor this reason, you need another level of abstraction to wrap, different processes/container that are functional related. A pod keeps the advantage of container isolation giving, the illusion to have all the processes running together.\nPods in Kubernetes can communicate with any other pod. When two pods send network packages between them, they see the IP address because no NAT gateways exists. It does not matter if two pods are in different namespaces or nodes, in both cases they can communicate each other.\n Figure 1. Network between pods. A pod can see other pods and it can be reachable, thanks to a routable IP address for each pod. Kubernetes in Action (2022)Â©  One or multiple containers in a pod ? First of all asking yourself the following questions:\n Do my processes represent a single whole or they are independent ? Do my processes need to run together (some node, that means same resources) or they could run on different hosts ? Do my processes need to scale together or independently ?  Let\u0026rsquo;s try to figure out the problem with a real case scenario. One of the most frequent cases is the web-app application case. The simplest infrastructure could be made from a Front-end (FE), Back-end (BE) and a Database.\nWe can organise this three pieces in different combinations. All in the same pod, three different pods, or two of them in a pod and the other in another pod. To find the best choice we have to consider few aspects. A good way to thinking on it, in the design phase, is make all the possible combinations and see pros and cons.\n  All in the same pod.\nIf all containers are in the same pod, they will always share the same machine. A pod is the unit of a Kubernetes resource, you cannot split the containers into. Another aspect to consider is that, if one of the containers has a failure the pods will send an error and this will hide also the other two services, also they are running correctly. The time of restart will also increase, typically BE and/or FE are update frequently, mostly during development phase. Instead, database is not subject to frequent updates. So, each time you want to see your changes, you should recreate a new pod, this brings a lot of overhead if also other services have to restart.\n  FE-BE in the same pod, Database alone.\nThis just exclude the overhead caused by the database in the init phase and also the pod with the database container could be deploy in a different node, with dedicated resources. Now, the main problem here is that the FE/BE still share the same machine, and, mostly, if they scale, they will scale together. This is a possible choice if the BE/FE are not supposed to be asymmetric, so none of them could have a workload which requires a lot of replicas of it. Suppose to have a BE which take time to process data, it is the perfect case in which you want to scale it, but anytime you also scale the FE, which is an unnecessary overhead.\n  Three different pods.\nGood choice for the case in which FE/BE could have a different workload. Resources can scale independently and can use all the available resources.\n  Keep in mind that not always a container per pod is the best choice, you should make a pros/cons list to have the best organisation for your architecture and do not going crazy to debug it after the deployment!\n","permalink":"https://kubetips.cloud/posts/containersinpod/","summary":"During the design phase you have to make a lot of right choices to avoid bad surprises during the deploy. One of the most frequent question is: \u0026ldquo;How can I organise my application in Kubernetes to achieve better performances ?\u0026rdquo; Sometimes the correct answer is just make the right decision in splitting and grouping the services in the correct way.\nPods, the resource you always need to Pods are the most used resource in a Kubernetes environment.","title":"Make the right choice: one or more Containers in a Pod ?"},{"content":"One of the most used service, to have your Kubernetes cluster deploy in cloud, is the Elastic Kubernetes Service provided by AWS.\nEKS, defined as containers-as-a-service (CaaS), (more details here regarding pricing and general overview) is a managed service that helps to run Kubernetes on AWS. Practically speaking, you do not need to manage the installation of Kubernetes on EC2 instances but you can leave all the configuration stuff to the services provided by EKS.\nConfiguration AWS CLI First of all you have to install and configure the AWS client, here the details for the installation. After that is necessary to add in the .aws folder (which should be created during the aws cli installation) two files:\n  config\naws_access_key_id=[get-from-account-settings] aws_secret_access_key=[get-from-account-settings] region=[aws-region] output=[format, default json]   credentials\naws_access_key_id=[get-from-account-settings] aws_secret_access_key=[get-from-account-settings]   Install kubectl Second step is to install the instrument you will use to administrate the cluster kubectl . Here the instruction to correctly download the right version of kubectl. Keep in mind that, after the installation, a .kube will appear on your home. This will contains the config file to connect with the correspondent cluster.\nAdd required IAM roles IAM roles in AWS environment identify the permissions policies that can be associated to an user. To add the correct permissions to your user, follow this guide. At the end your user should have all the permissions to create an EKS cluster from scratch.\nInstall eksctl Finally, you can install eksctl tool. As you have already done for kubectl, choose here the correct version, according with your OS.\nCreate your first cluster Now it is time to deploy your first cluster, using eksctl, it is trivial. The command is:\neksctl create cluster -f config-cluster.yaml\n#config-cluster.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cluster-name region: aws-region nodeGroups: - name: ec2-group-name desiredCapacity: # of instances instanceType: instance-type The deployment requires ~15 mins (could be different based on the number and the types of instances you choose). At the end, eksctl write in your .kube/config the specs to connect to the cluster when you use the kubectl tool. When the creation is done, you can interact with your cluster, for example get the system pods with:\nkubectl get pods -A\nThat\u0026rsquo;s all! Have fun with your EKS cluster.\n","permalink":"https://kubetips.cloud/posts/k8scloud/","summary":"One of the most used service, to have your Kubernetes cluster deploy in cloud, is the Elastic Kubernetes Service provided by AWS.\nEKS, defined as containers-as-a-service (CaaS), (more details here regarding pricing and general overview) is a managed service that helps to run Kubernetes on AWS. Practically speaking, you do not need to manage the installation of Kubernetes on EC2 instances but you can leave all the configuration stuff to the services provided by EKS.","title":"Kubernetes in Cloud: easy way to deploy an EKS cluster (AWS)"},{"content":"In this article I want to summarize the most used softwares to deploy a local Kubernetes cluster on your local machine. Having a local deployment of your infrastructure is always a good idea for testing and debugging. There are several platforms you can choose to quickly have your self-hosted Kubernetes.\nMinikube Minikube is probably the most used choice. It can runs in all the major OS and offers an easy installation guide. It is perfect for people that are using multiple OS machines and have some basic familiarity with Kubernetes and Docker. It is a good choice if the infrastructure you want to deploy is not too much complex. Otherwise, you will probably spend too much time looking for the correct minikube configuration, instead of working on your deployment.\nMain features that caracterized Minikube are :\n Supports for all the Kubernetes (good for very retro-compatibility) Cross-platform (Windows, MacOS, Linux) Different containerization systems (Docker, containerd, CRI-O) Add-ons to easily install Kubernetes features and properly configure the deployment Minikube itself could be install in a container as well as on bare metal.  Kind Kind (Kubernetes in Docker) is a CNCF (Cloud Native Computing Foundation) certified conformant Kubernetes installer. It is more devoted to test Kubernetes itself or to have a deeper look on how Kubernetes works under the hood. Kind gives you the possibility to easily install a local cluster using Docker containers as nodes. This makes easy to install, remove, upgrade it unlike Minikube.\nMain aspects are :\n Multi platform (Linux, MacOS, Windows) Multi node cluster (in Minikube you have just one node) No need to configure anything outside of Docker  Microk8s Microk8s is also a CNCF certifided installer for Kubernetes. It is a fully contained platform, all the pieces needed are wrapped in a single package. For this reason it is isolated from other tools of your machine. As for kind, also Microk8s is very easy to install and remove. It is relatively new, so it could be unstable, but seems promising.\nMain points of Microk8s :\n Multi platform BUT on Linux via snap, on MacOs via brew and using an installer for Windows Multi node cluster, each Linux instances connected to the master could be a node of the cluster. Add-ons, as for Minikube there are a set of customizable features.  ","permalink":"https://kubetips.cloud/posts/selfhostedk8s/","summary":"In this article I want to summarize the most used softwares to deploy a local Kubernetes cluster on your local machine. Having a local deployment of your infrastructure is always a good idea for testing and debugging. There are several platforms you can choose to quickly have your self-hosted Kubernetes.\nMinikube Minikube is probably the most used choice. It can runs in all the major OS and offers an easy installation guide.","title":"Kubernetes: solutions to deploy it locally"},{"content":"Hi there! I\u0026rsquo;m Marco Franzon and this is KubeTips.\nI would like to have a place in which share what I find interesting and curious using frequently Docker and Kubernetes. As I spend mostly of my time writing Python code and doing DevOps stuff with containers, everyday I face out new bugs or challenges to solve, sometimes it is fun sometimes it is frustrating, but always I learn something new.\nI think on this website as a \u0026ldquo;playbook\u0026rdquo; more than a blog. I will tell about my troubles with Docker, Podman, Singularity, Kubernetes, CRI-O, containerd and some useful tips or software to use to avoid sleepless nights!\nContact Me Want to have a chat with me? Feel free to drop a DM!\nThese are the platforms you can find me on:\n Linkedin: https://www.linkedin.com/in/marco-franzon/ Twitter: @marcofranzon_ap  ","permalink":"https://kubetips.cloud/about/","summary":"Hi there! I\u0026rsquo;m Marco Franzon and this is KubeTips.\nI would like to have a place in which share what I find interesting and curious using frequently Docker and Kubernetes. As I spend mostly of my time writing Python code and doing DevOps stuff with containers, everyday I face out new bugs or challenges to solve, sometimes it is fun sometimes it is frustrating, but always I learn something new.","title":"About Me"},{"content":"The main philosophy of this website is to share the knowledge between developers that have found new solutions for everyday problems.\nIf you want to tell your \u0026ldquo;how to\u0026rdquo;, or you think that an article is incomplete or you can do it better, you are in the right place. Click on Github to start.\n Clone the repo Add your post in ./kubetips/content/posts or modify an existing one.  Posts are written in MarkDown, use this as header in case you are writing a new one: --- title: \u0026quot;Title\u0026quot; date: {{ .Date }} draft: true author : yourname --- If you want to modify an existing one, just add your name in header's authors list Make a Pull Request  And that\u0026rsquo;s all! As soon as possible I will have a look on the new post and for sure merge your contribution.\n","permalink":"https://kubetips.cloud/contribute/","summary":"The main philosophy of this website is to share the knowledge between developers that have found new solutions for everyday problems.\nIf you want to tell your \u0026ldquo;how to\u0026rdquo;, or you think that an article is incomplete or you can do it better, you are in the right place. Click on Github to start.\n Clone the repo Add your post in ./kubetips/content/posts or modify an existing one.  Posts are written in MarkDown, use this as header in case you are writing a new one: --- title: \u0026quot;Title\u0026quot; date: {{ .","title":"How to contribute"}]